# Environment Configuration for Whisper Voice Command System
# Copy this file to .env and customize for your setup

# =============================================================================
# WHISPER MODEL CONFIGURATION
# =============================================================================

# Whisper model size to use
# Options: tiny (39M), base (74M), small (244M), medium (769M), large (1550M)
# Recommended: base (good balance of speed and accuracy)
# Note: Larger models require more RAM and have higher latency
WHISPER_MODEL_SIZE=base

# Confidence threshold for publishing commands (0.0-1.0)
# Commands below this threshold will be logged but not published
# Recommended: 0.7 for general use, 0.8 for critical safety commands
CONFIDENCE_THRESHOLD=0.7

# =============================================================================
# AUDIO CONFIGURATION
# =============================================================================

# Audio sample rate in Hz
# Whisper is trained on 16kHz audio - do not change unless necessary
SAMPLE_RATE=16000

# Microphone device index
# -1 for default system microphone
# To find your microphone index, run: python -m sounddevice
# Then set this to the index number (e.g., 0, 1, 2, etc.)
DEVICE_INDEX=-1

# Recording duration in seconds for each audio chunk
# Shorter = lower latency but may cut off long commands
# Longer = better accuracy for complex commands but higher latency
# Recommended: 3.0 seconds
RECORDING_DURATION=3.0

# =============================================================================
# ROS 2 CONFIGURATION
# =============================================================================

# ROS 2 Domain ID (must match other nodes in your system)
# Default: 0
ROS_DOMAIN_ID=0

# Voice commands topic name
# The node publishes transcriptions to this topic
VOICE_COMMANDS_TOPIC=/voice_commands

# =============================================================================
# DEVELOPMENT & DEBUGGING
# =============================================================================

# Enable debug logging (true/false)
# When true, logs all transcriptions including low-confidence ones
DEBUG_MODE=false

# Log file path (optional)
# Leave empty to log to console only
# Example: /tmp/voice_commands.log
LOG_FILE_PATH=

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# Number of threads for Whisper inference (0 = auto-detect)
# Higher values may improve performance on multi-core systems
# Recommended: 4 for typical laptops, 8+ for workstations
NUM_THREADS=0

# Enable FP16 inference for faster processing (requires GPU with CUDA)
# Set to true only if you have a compatible NVIDIA GPU
# Warning: May reduce accuracy slightly
USE_FP16=false

# =============================================================================
# TESTING CONFIGURATION
# =============================================================================

# Path to test command list for accuracy validation
# Used by test scripts to validate >90% accuracy requirement
TEST_COMMANDS_FILE=config/voice_commands.yaml

# Minimum required accuracy for test pass (0.0-1.0)
# Module requirement: 0.90 (90%)
MIN_TEST_ACCURACY=0.90

# =============================================================================
# NOTES
# =============================================================================

# 1. MICROPHONE PERMISSIONS
#    - Ubuntu: Settings → Privacy → Microphone → Enable for terminal
#    - macOS: System Preferences → Security & Privacy → Microphone
#    - Windows: Settings → Privacy → Microphone → Allow apps
#
# 2. DEVICE INDEX DETECTION
#    Run this to find your microphone:
#    python -c "import sounddevice as sd; print(sd.query_devices())"
#
# 3. MODEL DOWNLOAD
#    Whisper models auto-download on first use (~140MB for base model)
#    Stored in: ~/.cache/whisper/
#
# 4. PERFORMANCE EXPECTATIONS
#    Model    | Params | Size  | Latency (CPU) | Accuracy
#    ---------|--------|-------|---------------|----------
#    tiny     | 39M    | 72MB  | 1-2s          | ~80%
#    base     | 74M    | 140MB | 2-4s          | ~90%
#    small    | 244M   | 461MB | 4-8s          | ~93%
#    medium   | 769M   | 1.5GB | 10-15s        | ~95%
#    large    | 1550M  | 2.9GB | 20-30s        | ~96%
#
#    Latency on GPU (CUDA): ~50-70% faster than CPU
