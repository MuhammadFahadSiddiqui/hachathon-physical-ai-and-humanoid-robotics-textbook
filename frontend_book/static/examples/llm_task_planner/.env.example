# Environment Configuration for LLM Task Planner
# Copy this file to .env and customize for your setup

# =============================================================================
# LLM BACKEND SELECTION
# =============================================================================

# Which LLM backend to use
# Options: "openai" (GPT-4 API, paid), "ollama" (Llama 3.1, free local)
LLM_BACKEND=ollama

# =============================================================================
# OPENAI CONFIGURATION (only needed if LLM_BACKEND=openai)
# =============================================================================

# OpenAI API Key
# Get from: https://platform.openai.com/api-keys
# Requires: Phone number, credit card (pay-as-you-go)
OPENAI_API_KEY=sk-your-api-key-here

# OpenAI Model Selection
# Options: gpt-4-turbo (recommended), gpt-4, gpt-3.5-turbo
# gpt-4-turbo: ~$0.03/request, 95-98% valid plans, <5s latency
# gpt-4: ~$0.06/request, 95-98% valid plans, <10s latency
# gpt-3.5-turbo: ~$0.002/request, 85-90% valid plans, <3s latency
OPENAI_MODEL=gpt-4-turbo

# =============================================================================
# OLLAMA CONFIGURATION (only needed if LLM_BACKEND=ollama)
# =============================================================================

# Ollama Server URL
# Default: http://localhost:11434 (local Ollama server)
# For remote: http://your-server-ip:11434
OLLAMA_HOST=http://localhost:11434

# Ollama Model Selection
# Options: llama3.1:8b (recommended), llama3.1:70b, mistral:7b
# llama3.1:8b: Free, 90-93% valid plans, 5-10s latency (CPU), 2-5s (GPU)
# llama3.1:70b: Free, 93-96% valid plans, 30-60s latency (requires 40GB+ RAM)
# mistral:7b: Free, 85-90% valid plans, 3-7s latency
OLLAMA_MODEL=llama3.1:8b

# =============================================================================
# LLM PARAMETERS
# =============================================================================

# Temperature (0.0-1.0)
# 0.0 = Deterministic (recommended for robotics - same input â†’ same output)
# 1.0 = Creative (NOT recommended for task planning)
LLM_TEMPERATURE=0.0

# Maximum tokens in response
# Typical action plans: 50-300 tokens
# Complex plans: 300-500 tokens
MAX_TOKENS=500

# API timeout in seconds
# Increase if using slow network or large Ollama model
TIMEOUT=30.0

# Maximum retries for invalid plans
# Planner will retry up to this many times before giving up
MAX_RETRIES=3

# =============================================================================
# ROS 2 CONFIGURATION
# =============================================================================

# ROS 2 Domain ID (must match other nodes)
ROS_DOMAIN_ID=0

# Input topic (from Whisper voice commands)
VOICE_COMMANDS_TOPIC=/voice_commands

# Output topic (to behavior coordinator)
TASK_PLAN_TOPIC=/task_plan

# =============================================================================
# SETUP INSTRUCTIONS
# =============================================================================

# OPTION 1: Ollama (FREE, LOCAL)
# 1. Install Ollama:
#    curl -fsSL https://ollama.com/install.sh | sh
#
# 2. Pull model:
#    ollama pull llama3.1:8b
#
# 3. Start server:
#    ollama serve
#
# 4. Test connection:
#    curl http://localhost:11434/api/tags
#
# 5. Set environment:
#    export LLM_BACKEND=ollama
#    export OLLAMA_MODEL=llama3.1:8b

# OPTION 2: OpenAI GPT-4 (PAID, CLOUD)
# 1. Sign up at https://platform.openai.com
#
# 2. Add payment method (pay-as-you-go, ~$0.03/request)
#
# 3. Create API key: https://platform.openai.com/api-keys
#
# 4. Set environment:
#    export OPENAI_API_KEY='sk-...'
#    export LLM_BACKEND=openai
#    export OPENAI_MODEL=gpt-4-turbo

# =============================================================================
# COST ESTIMATION (as of 2024)
# =============================================================================

# OpenAI GPT-4 Turbo Pricing:
# - Input: $0.01 per 1K tokens
# - Output: $0.03 per 1K tokens
# - Average request: 500 input + 200 output = ~$0.017 per plan
# - Full module (100 test commands): ~$1.70

# Ollama (all models):
# - Cost: $0 (runs locally)
# - Hardware: 16GB+ RAM for 8B models, 40GB+ for 70B models

# =============================================================================
# PERFORMANCE BENCHMARKS
# =============================================================================

# System: Intel i7-10700K (8-core CPU), 32GB RAM, No GPU

# Ollama llama3.1:8b (CPU):
# - Simple commands (1-2 actions): 5-10s
# - Complex commands (5-8 actions): 15-20s
# - Valid plan rate: 92%

# OpenAI gpt-4-turbo (API):
# - Simple commands: 2-5s
# - Complex commands: 5-15s
# - Valid plan rate: 97%

# =============================================================================
# TROUBLESHOOTING
# =============================================================================

# Issue: "Connection refused" when using Ollama
# Solution: Ensure Ollama server is running
#   ollama serve

# Issue: "Model not found" error
# Solution: Pull model first
#   ollama pull llama3.1:8b

# Issue: "Invalid API key" with OpenAI
# Solution: Check API key is correctly set
#   echo $OPENAI_API_KEY  # Should show sk-...

# Issue: Plans take >30s
# Solution: Increase timeout or use smaller model
#   export TIMEOUT=60.0
#   export OLLAMA_MODEL=mistral:7b  # Faster than llama3.1:8b

# Issue: Low valid plan rate (<80%)
# Solution: Use better model or adjust temperature
#   export OLLAMA_MODEL=llama3.1:70b  # Better quality
#   export LLM_TEMPERATURE=0.0  # More deterministic
